## Set Up Environment

```
conda create --name your_env_name python=3.6
conda activate your_env_name
pip install -r requirements.txt
```

## Dataset

The distance maps is generated by `Gen_BDist_Map.ipynb`, see [details](./Gen_BDist_Map.ipynb)

Pathes is extracted by `Extract_Patches_Own_data.ipynb`, see [details](./Extract_Patches_Own_data.ipynb)

Download the ccRCC grading dataset which has been pre-processed in our paper from [this link](https://pennstateoffice365-my.sharepoint.com/:f:/g/personal/hzz5333_psu_edu/Ej_-GE4M-1NCibvnn7MsleABSTDOs262mTAIO2g62NgeNg?e=XyWDwa).

## Choosing the network

The model to use and the selection of other hyperparameters is selected in `config.py`. The models available are:
- Ournetwork: `model/mynetwork.py`
- HoVer-Net: `model/graph.py`
- Micro-Net: `model/micronet.py`
- U-Net: `model/unet.py`
- FCN8: `model/fcn8.py`


## Modifying Hyperparameters

To modify hyperparameters, refer to `opt/`. For CHR-Net, modify the script `opt/other.py`.

## Augmentation

To modify the augmentation pipeline, refer to `get_train_augmentors()` in `config.py`. Refer to [this webpage](https://tensorpack.readthedocs.io/modules/dataflow.imgaug.html)        for information on how to modify the augmentation parameters.

## Training

To train the network:

Run the `Trainning.ipynb`.

Set the `gpus`, which denotes which GPU will be used for training.

Before training, set in `config.py` and `opt/other.py`:
- path to pretrained weights ResNet34. Download the weights [here](https://pennstateoffice365-my.sharepoint.com/:f:/g/personal/hzz5333_psu_edu/Eosuj25fa9tJig6KYsuKJ5oBWFkDdhPj79dZPSVXPnzpFg?e=8Xcrsb).
- path to the data directories
- path where checkpoints will be saved

## Inference

To do the inference:

Run the `Predicting.ipynb`.

Set the `gpus`, which denotes which GPU will be used for inference.

## Result
The visiable results of our net work can be downloaded [here](https://pennstateoffice365-my.sharepoint.com/:f:/g/personal/hzz5333_psu_edu/EicOpxB4raZGjSlDoTbkP94BOs6rFBz0aVPXwXAHlyu3hQ?e=dKoJhc)
